{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11429933,"sourceType":"datasetVersion","datasetId":7158722}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install torch transformers tqdm scikit-learn nltk rouge-score hf_xet SPARQLWrapper matplotlib -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-05-31T11:27:16.072Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.optim import AdamW\nfrom transformers import (\n    AutoModelForSeq2SeqLM,\n    AutoTokenizer,\n    get_linear_schedule_with_warmup,\n    AutoConfig\n)\nfrom tqdm import tqdm\nimport nltk\nimport numpy as np\nimport os\nimport re\nfrom nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\nfrom rouge_score import rouge_scorer\n\nnltk.download('punkt', quiet=True)\n\nMODEL_NAME = \"google/flan-t5-base\"\nMAX_SOURCE_LENGTH = 128\nMAX_TARGET_LENGTH = 256\nBATCH_SIZE = 16\nACCUMULATION_STEPS = 1\nLEARNING_RATE = 2e-5\nWEIGHT_DECAY = 0.01\nNUM_EPOCHS = 100\nEARLY_STOP_PATIENCE = 10\nLENGTH_PENALTY = 1.1\nNUM_BEAMS = 6\nNO_REPEAT_NGRAM_SIZE = 3\nDATASET_PATH = \"/kaggle/input/lcquad2/train.json\"\nCHECKPOINT_DIR = \"/kaggle/working/checkpoints\"\nPERSISTENT_LOAD_CHECKPOINT = \"/kaggle/input/my_checkpoints/latest.pt\"\nWORKING_CHECKPOINT = os.path.join(CHECKPOINT_DIR, \"latest.pt\")\nAUGMENT_PARAPHRASE = True\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif torch.cuda.is_available():\n    torch.backends.cudnn.benchmark = True\n    tqdm.write(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\nelse:\n    tqdm.write(\"Using CPU\")\n\ndef normalize_sparql(query):\n    return re.sub(r'\\s+', ' ', query.strip().lower())\n\nclass KBQADataset(Dataset):\n    def __init__(self, data, tokenizer, max_source_length, max_target_length, augment_paraphrase=False):\n        self.samples = []\n        for item in data:\n            question = item.get('question') or \"\"\n            sparql = item.get('sparql_wikidata') or \"\"\n            self.samples.append((question, sparql))\n            if augment_paraphrase:\n                paraphrased = item.get('paraphrased_question')\n                if paraphrased and paraphrased.strip() and paraphrased.strip() != question.strip():\n                    self.samples.append((paraphrased, sparql))\n        self.tokenizer = tokenizer\n        self.max_source_length = max_source_length\n        self.max_target_length = max_target_length\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self, idx):\n        question, sparql = self.samples[idx]\n        src_enc = self.tokenizer(\n            f\"generate query: {question}\",\n            max_length=self.max_source_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        tgt_enc = self.tokenizer(\n            sparql,\n            max_length=self.max_target_length,\n            padding='max_length',\n            truncation=True,\n            return_tensors=\"pt\"\n        )\n        labels = tgt_enc['input_ids'].squeeze().clone()\n        labels[labels == self.tokenizer.pad_token_id] = -100\n        return {\n            'input_ids': src_enc['input_ids'].squeeze(),\n            'attention_mask': src_enc['attention_mask'].squeeze(),\n            'labels': labels,\n            'target_text': sparql\n        }\n\ndef load_dataset():\n    if not os.path.exists(DATASET_PATH):\n        raise FileNotFoundError(f\"Dataset not found at {DATASET_PATH}.\")\n    with open(DATASET_PATH, 'r') as f:\n        return json.load(f)\n\ndef train_epoch(model, dataloader, optimizer, scheduler, device):\n    model.train()\n    total_loss = 0.0\n    optimizer.zero_grad()\n    for i, batch in enumerate(tqdm(dataloader, desc=\"Training\", leave=True)):\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n\n        outputs = model(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            labels=labels\n        )\n        loss = outputs.loss.mean() / ACCUMULATION_STEPS\n        loss.backward()\n\n        if (i + 1) % ACCUMULATION_STEPS == 0:\n            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.25)\n            optimizer.step()\n            scheduler.step()\n            optimizer.zero_grad()\n\n        total_loss += loss.item() * ACCUMULATION_STEPS\n    return total_loss / len(dataloader)\n\ndef main():\n    tqdm.write(f\"Using device: {DEVICE}\")\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n    model_config = AutoConfig.from_pretrained(MODEL_NAME)\n    model_config.dropout_rate = 0.1\n    model_config.attention_dropout_rate = 0.1\n    model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, config=model_config)\n    model.gradient_checkpointing_enable()\n    model.to(DEVICE)\n\n    data = load_dataset()\n    train_ds = KBQADataset(data, tokenizer, MAX_SOURCE_LENGTH, MAX_TARGET_LENGTH, augment_paraphrase=AUGMENT_PARAPHRASE)\n    train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n\n    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n    total_steps = len(train_dl) * NUM_EPOCHS\n    warmup_steps = int(0.03 * total_steps)\n    scheduler = get_linear_schedule_with_warmup(optimizer, warmup_steps, total_steps)\n\n    start_epoch = 1\n    best_bleu = -1\n    stop_counter = 0\n\n    os.makedirs(CHECKPOINT_DIR, exist_ok=True)\n    checkpoint_loaded = False\n    if os.path.exists(PERSISTENT_LOAD_CHECKPOINT):\n        tqdm.write(f\"Resuming from persistent checkpoint {PERSISTENT_LOAD_CHECKPOINT}\")\n        checkpoint = torch.load(PERSISTENT_LOAD_CHECKPOINT, map_location=DEVICE)\n        checkpoint_loaded = True\n    elif os.path.exists(WORKING_CHECKPOINT):\n        tqdm.write(f\"Resuming from working checkpoint {WORKING_CHECKPOINT}\")\n        checkpoint = torch.load(WORKING_CHECKPOINT, map_location=DEVICE)\n        checkpoint_loaded = True\n\n    if checkpoint_loaded:\n        model.load_state_dict(checkpoint['model_state'])\n        optimizer.load_state_dict(checkpoint['optim_state'])\n        scheduler.load_state_dict(checkpoint['sched_state'])\n        start_epoch = checkpoint['epoch'] + 1\n        best_bleu = checkpoint.get('best_bleu', -1)\n        stop_counter = checkpoint.get('stop_counter', 0)\n        tqdm.write(f\"Resumed at epoch {start_epoch}\")\n    else:\n        tqdm.write(\"No checkpoint found, starting from scratch.\")\n\n    for epoch in range(start_epoch, NUM_EPOCHS + 1):\n        tqdm.write(f\"Starting Epoch {epoch}/{NUM_EPOCHS}\")\n        tr_loss = train_epoch(model, train_dl, optimizer, scheduler, DEVICE)\n        tqdm.write(f\"Epoch {epoch}: Train Loss: {tr_loss:.4f}\")\n\n        if epoch % 2 == 0 or epoch == NUM_EPOCHS:\n            torch.save({\n                'model_state': model.state_dict(),\n                'optim_state': optimizer.state_dict(),\n                'sched_state': scheduler.state_dict(),\n                'epoch': epoch,\n                'best_bleu': best_bleu,\n                'stop_counter': stop_counter\n            }, WORKING_CHECKPOINT)\n            tqdm.write(f\"Checkpoint saved at epoch {epoch}\")\n\n    output_dir = \"./t5_model\"\n    os.makedirs(output_dir, exist_ok=True)\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-05-31T11:27:16.073Z"}},"outputs":[],"execution_count":null}]}